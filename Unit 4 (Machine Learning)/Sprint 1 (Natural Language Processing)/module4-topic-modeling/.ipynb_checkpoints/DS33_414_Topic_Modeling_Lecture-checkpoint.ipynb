{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOMBkFKn6uEK"
      },
      "source": [
        "# Topic Modeling (Prepare)\n",
        "\n",
        "On Monday we talked about summarizing your documents using just token counts. Today, we're going to learn about a much more sophisticated approach - learning 'topics' from documents. Topics are a latent structure. They are not directly observable in the data, but we know they're there by reading them.\n",
        "\n",
        "> **latent**: existing but not yet developed or manifest; hidden or concealed.\n",
        "\n",
        "## Use Cases\n",
        "Primary use case: What the are your documents about? Who might want to know that in industry - \n",
        "* Identifying common themes in customer reviews\n",
        "* Grouping job ads into categories \n",
        "* Monitoring communications (Email - State Department, Google) \n",
        "\n",
        "## Learning Objectives\n",
        "*At the end of the lesson you should be able to:*\n",
        "* Part 1: Describe how an LDA Model works\n",
        "* Part 2: Build a LDA Model with Gensim\n",
        "* Part 3: Interpret LDA results & Select the appropriate number of topics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# could use *web_lg or *web_sm instead\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI3i8KbH1NWT",
        "outputId": "b3fa9696-ffdc-4e8e-f990-58f23d923846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051301 sha256=8d0823ccd6337a849ffc8ed68bc5c5b0b28186514cc2ccb812ab5ce37b52e59c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qju2ks_e/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart Runtime!"
      ],
      "metadata": {
        "id": "qFQJ7N9n1eDa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re9Q9Vgw0FS0",
        "outputId": "b07b8cdf-f9aa-49d6-ceea-560a8ab87981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandarallel==1.4.8\n",
            "  Downloading pandarallel-1.4.8.tar.gz (14 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from pandarallel==1.4.8) (0.3.4)\n",
            "Building wheels for collected packages: pandarallel\n",
            "  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandarallel: filename=pandarallel-1.4.8-py3-none-any.whl size=16111 sha256=3c4a2fa43ed6763bc1ede86a942721f381934fb1a7fae7e7556e0f6a735d1c0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f2/4e/e40c8b9344cccf6b8a02d8d8808ba837e72b607c4be946878a\n",
            "Successfully built pandarallel\n",
            "Installing collected packages: pandarallel\n",
            "Successfully installed pandarallel-1.4.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pandarallel==1.4.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqJQ4AlA0FS4",
        "outputId": "1140f4c3-6c1b-4bac-a37e-1dd9b9414e1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149870 sha256=e0faa7bf7987256f909298c7d6171e9e2c2114c9d6b6b753ddcbc874abf2e944\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Up2SjZJJ0jGo",
        "outputId": "1491ca73-c4fb-469c-d38b-aa5016b7dec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 378 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 389 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 399 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 419 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 430 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 440 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 450 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 471 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 481 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 491 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 501 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 532 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 542 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 552 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 583 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 593 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 604 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 614 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 624 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 634 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 645 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 655 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 665 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 675 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 686 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 696 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 706 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 716 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 727 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 737 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 747 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 757 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 768 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 778 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 788 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 798 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 808 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 819 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 829 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 839 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 849 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 860 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 870 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 880 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 890 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 901 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 911 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 921 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 931 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 942 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 952 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 962 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 972 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 983 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 993 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Collecting numpy>=1.20.0\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Collecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis) (3.0.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.0.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=572d5acf96f6302c5dee114e5000dd4cf0c25c2acf997d04e73d489a3d09cc23\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.17 numpy-1.21.5 pandas-1.3.5 pyLDAvis-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnyNg8sQ6uEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36bd345c-b342-4f4f-aa5a-4be7f5682361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "import spacy\n",
        "spacy.util.fix_random_seed(0)\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZjG6U86uEe"
      },
      "source": [
        "# Part 1: Describe how an LDA Model works\n",
        "\n",
        "We are going to focus on a high level of understand for how LDA works, meaning we are going to focus on \"what it does\" instead of \"how it does it\". I realize that this may be unsatisfying to some so I've included some resources that serve as a prerequisite for understanding how LDA works at a mathematical level. \n",
        "\n",
        "LDA is a [**Probabilistic Graphical Model (PGM)**](https://en.wikipedia.org/wiki/Graphical_model)\n",
        "\n",
        "PGM are represented by a graph that expresses the conditional dependence structure between random variables. Here's the LDA representation dependency graph: \n",
        "\n",
        "![](https://filebox.ece.vt.edu/~s14ece6504/projects/alfadda_topic/main_figure_3.png)\n",
        "\n",
        "These image is communicating the hierarchical dependency between probability distributions and their parameters. This is an application of Bayesian Probability - on steroids. \n",
        "\n",
        "\n",
        "In order to understand how LDA works, one must first understand how PGM work. If this is something that you're interested in learning more about here are some resources: \n",
        "\n",
        "This Github repo that has transformed a textbook in a collection of Jupyter Notebooks. This repo is call [**\"Bayesian Methods for Hackers\"**](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) The cool thing about this repo is that each chapter has the same material covered in several notebooks but each notebook is written in a different python package: **PyMC2, PyMC3, Pyro, and Tensorflow Probability.** So you can even learn a new library if you want or stick to what you know. \n",
        "\n",
        "Having said that, [**Pyro**](https://pyro.ai/) is considered a very powerful probabilistic programming library that even combines probabilistic programming with deep learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CB9mbKa0FS7"
      },
      "source": [
        "### Resources for LDA\n",
        "\n",
        "[**Your Guide to Latent Dirichlet Allocation**](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d) Here's a medium article that works through an example of LDA. This is useful if you'd like to get an exposure of LDA outside of this notebook.\n",
        "\n",
        "[**LDA Topic Modeling**](https://lettier.com/projects/lda-topic-modeling/)This is an interactive data visualization tool that allows us to explore a simple and visual example of LDA. We'll be using this to learn about LDA in class. \n",
        "\n",
        "[**Topic Modeling with Gensim**](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) This is an example of implementing LDA using the same dataset that we are using in the guided project.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3mOL6_Q0FS7"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "We are going to load some emails. Those emails do belong to topics however those topics are hierarchical.\n",
        "\n",
        "    sci\n",
        "        \\_ electronics, space\n",
        "\n",
        "\n",
        "    talk\n",
        "        \\_politics \n",
        "                  \\_ guns, middle east\n",
        "              \n",
        "So what's the best way to categorize these emails - is it between science and talk? \n",
        "\n",
        "Is it between electronics, space, guns, and the Middle East? \n",
        "\n",
        "The Middle East is a pretty broad topics in and of itself, should that topic be broken down into further sub-topics?\n",
        "\n",
        "\n",
        "Let's learn about Topic Modeling and how it can help us answer this questions!\n",
        "\n",
        "### Load Email Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlmFvnTE0FS8"
      },
      "outputs": [],
      "source": [
        "data = fetch_20newsgroups()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW8phYt-0FS9",
        "outputId": "108e6dfe-cda0-4c32-e2bf-67dca0da9fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sci.electronics', 'sci.space', 'talk.politics.guns', 'talk.politics.mideast']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data.target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJqcYfES0FS-"
      },
      "outputs": [],
      "source": [
        "# notice that the categories are hierarchical\n",
        "# so there is a sense in which we 2 topics but also as many as 4 topics  \n",
        "categories = ['sci.electronics', 'sci.space', \n",
        "              'talk.politics.guns', 'talk.politics.mideast']\n",
        "data = fetch_20newsgroups(categories=categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pru3v4ua0FS-"
      },
      "outputs": [],
      "source": [
        "np.unique(data.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "53a96e4bd0cab21c17f1baa88b5699d8",
          "grade": false,
          "grade_id": "cell-3240b1c818e9dcc2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "PfQsTarM0FS_"
      },
      "outputs": [],
      "source": [
        "# create X and Y from data\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "target_names = data.target_names\n",
        "\n",
        "target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaMsy1XAGLxc"
      },
      "outputs": [],
      "source": [
        "# loda data into a dataframe \n",
        "\n",
        "data = {\n",
        "    'content': X,\n",
        "    'target': y,\n",
        "    'target_names': [target_names[target_id] for target_id in y]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data=data)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "na2bkOcFGter",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "839eeeb300650deafafc2dcd19a9e597",
          "grade": false,
          "grade_id": "cell-6ce62b4e40acee5f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def clean_data(text):\n",
        "    \n",
        "    # remove newline characters\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    \n",
        "    # remove email addresses\n",
        "    # from the beginning of the text\n",
        "    text = re.sub('From: \\S*@\\S*\\s?', ' ', text)\n",
        "    # from the middle of the text\n",
        "    text = re.sub('\\S*@\\S*\\s?', ' ', text)\n",
        "    \n",
        "    # remove apostrophes\n",
        "    text = re.sub(\"'\", '', text)\n",
        "    \n",
        "    # remove punctuation\n",
        "    text = re.sub('[^a-zA-Z 0-9]', ' ', text)\n",
        "    \n",
        "    # remove \"Reply To\"\n",
        "    text = re.sub(\"Reply To\", ' ', text)\n",
        "    \n",
        "    # remove newline characters\n",
        "    # everything that is more than one space\n",
        "    # replace it with just a single space\n",
        "    text = re.sub(\"[ ]{2,}\", ' ', text)\n",
        "    \n",
        "    return text.lower().strip()\n",
        "    \n",
        "# clean_data(df['content'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARLajQVA0FTB"
      },
      "outputs": [],
      "source": [
        "# clean our text data and save it to a new column\n",
        "df[\"clean_data\"] = df[\"content\"].apply(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCftuW6t0FTB"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGzm_97p0FTB"
      },
      "source": [
        "### Create Tokens \n",
        "\n",
        "Before we can use the Gemsim library to create bag-of-words vectors in exactly the right way that the LDA model wants, we must first create tokens. \n",
        "\n",
        "Let's use spaCy to create some lemmas. But first let's initialize our multi-processing library `pandarallel` which will empower us to use the same dataframe that our data is stored in but be able to create tokens in parallel so as to save time.\n",
        "\n",
        "Here's the documentation for [**pandarallel**](https://github.com/nalepae/pandarallel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qSxPV6e0FTC"
      },
      "outputs": [],
      "source": [
        "# we mush initalize pandarallel before we can use it\n",
        "pandarallel.initialize(progress_bar=True, nb_workers=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veskPVb00FTC"
      },
      "outputs": [],
      "source": [
        "# load in our spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8ntMU3R0FTC"
      },
      "outputs": [],
      "source": [
        "def tokenizer(text):\n",
        "    lemmas = []\n",
        "    for token in nlp(text):\n",
        "        if (token.is_stop != True) and (token.is_punct != True) and (token.is_space !=True):\n",
        "            lemmas.append(token.lemma_.lower())    \n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EBPQXqEKE9P"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# create our tokens in the form of lemmas \n",
        "df['lemmas'] = df['clean_data'].parallel_apply(tokenizer)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6v4e_dA0FTD"
      },
      "source": [
        "### Take a look at our lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8UnBeYs0FTD"
      },
      "outputs": [],
      "source": [
        "# print out the lemmeas from the first article\n",
        "# I'm just using np.array() to make the code cell output shorter\n",
        "np.array(df['lemmas'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vg4xT330FTE"
      },
      "source": [
        "### Filter out low quality lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVtDND600FTE"
      },
      "outputs": [],
      "source": [
        "def filter_lemmas(lemmas):\n",
        "    \"\"\"\n",
        "    Filter out any lemmas that are 2 characters or smaller\n",
        "    \"\"\"\n",
        "    return [lemmas for lemmas in lemmas if len(lemmas) > 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvBuJKRb0FTE"
      },
      "outputs": [],
      "source": [
        "df['lemmas'] = df['lemmas'].apply(filter_lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqYTbQh80FTE"
      },
      "outputs": [],
      "source": [
        "np.array(df['lemmas'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FuCwN00FTF"
      },
      "source": [
        "### The two main inputs to the LDA topic model are the dictionary (id2word) and the corpus. (Build our Gensim-style Document-term-matrix (corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "1klqRpqtJxWc",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dd66ab4eff9f69a3e9be0aa57fdc2598",
          "grade": false,
          "grade_id": "cell-79d38b90e5c6e38b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Create Dictionary\n",
        "# A lot like ounter from monday\n",
        "\n",
        "# Term Document Frequency\n",
        "# creating the vocabulary of sorts (assigning integers to each unique token)\n",
        "id2word = corpora.Dictionary(df['lemmas'])\n",
        "\n",
        "# stores (token id, token count) for each doc in the corpus\n",
        "corpus = [id2word.doc2bow(text) for text in df['lemmas']]\n",
        "\n",
        "# Our corpus is now a sparse Document-Term Matrix\n",
        "# corpus\n",
        "\n",
        "doc_id=5\n",
        "# Human readable format of corpus (term-frequency)\n",
        "# looking at one row of our document-term matrix\n",
        "[(id2word[word_id], word_count) for word_id, word_count in corpus[doc_id]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NwFVxAO0FTF"
      },
      "source": [
        "# Part 2: Estimate a LDA Model with Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TTeGoGB0FTF"
      },
      "source": [
        " ### Train an LDA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z5DNugI0FTG"
      },
      "source": [
        "### Does the same thing as the parallelized version below, but is not actually paralellized. You will ned to use this version on the Sprint Challenge to train an LDA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fasvjf0VLQ2a"
      },
      "outputs": [],
      "source": [
        "# ## This cell runs the single-processor version of the model (slower)\n",
        "# %%time\n",
        "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "#                                            id2word=id2word,\n",
        "#                                            num_topics=20, \n",
        "#                                            chunksize=100,\n",
        "#                                            passes=10,\n",
        "#                                            per_word_topics=True)\n",
        "# lda_model.save('lda_model.model')\n",
        "# # https://radimrehurek.com/gensim/models/ldamodel.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Df76kOY0FTG"
      },
      "source": [
        "### pandarallel code: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSVmANK40FTG"
      },
      "outputs": [],
      "source": [
        "def get_lda(id2word, num_topics):\n",
        "    return gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                        id2word=id2word,\n",
        "                                                        num_topics=num_topics, \n",
        "                                                        chunksize=100,\n",
        "                                                        passes=10,# runtime related parameter\n",
        "                                                        per_word_topics=True,\n",
        "                                                        workers=10, # runtime related parameter\n",
        "                                                        random_state=1234, \n",
        "                                                        iterations=20) # runtime related parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkni-OZA0FTG"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "num_topics = 2\n",
        "### This cell runs the multi-processor version of the model (faster)\n",
        "lda_multicore_2_topics = get_lda(id2word, 2)\n",
        "\n",
        "lda_multicore_2_topics.save('lda_2.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51MYDJhU0FTH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "num_topics = 6\n",
        "### This cell runs the multi-processor version of the model (faster)\n",
        "lda_multicore_6_topics = get_lda(id2word, 6)\n",
        "\n",
        "lda_multicore_6_topics.save('lda_6.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSxpllfO0FTH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "num_topics = 4\n",
        "### This cell runs the multi-processor version of the model (faster)\n",
        "lda_multicore_4_topics = get_lda(id2word, 4)\n",
        "\n",
        "lda_multicore_4_topics.save('lda_6.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFqJsuL00FTI"
      },
      "outputs": [],
      "source": [
        "from gensim import models\n",
        "#lda_multicore =  models.LdaModel.load('lda_multicore.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg5f-hQm0FTI"
      },
      "source": [
        "# Part 3: Interpret LDA results & Select the appropriate number of topics\n",
        "\n",
        "Only works in localy Jupyter notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL2puw-h0FTI"
      },
      "outputs": [],
      "source": [
        "# allows interactive javascript applications to be run in the output of a code cell\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYXi480VLaHK"
      },
      "outputs": [],
      "source": [
        "# pyLDAvis.gensim has been renamed to pyLDAvis.gensim_models\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_multicore_2_topics, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vxcg_zt0FTJ"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_multicore_6_topics, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ic5tafH0FTJ"
      },
      "outputs": [],
      "source": [
        "# pyLDAvis.enable_notebook()\n",
        "# vis = pyLDAvis.gensim_models.prepare(lda_multicore_4_topics, corpus, id2word)\n",
        "# vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU6HgcIr0FTJ"
      },
      "outputs": [],
      "source": [
        "target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avseMhbr0FTJ"
      },
      "source": [
        "## What is topic coherence?\n",
        "Topic Coherence measures score a single topic by **measuring the degree of semantic similarity between high scoring words in the topic**. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference.\n",
        "\n",
        "\n",
        "A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is **“the game is a team sport”**, **“the game is played with a ball”**, **“the game demands great physical efforts”**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjtXk8J3LaXC"
      },
      "outputs": [],
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                        id2word=id2word,\n",
        "                                                        num_topics=num_topics, \n",
        "                                                        chunksize=100,\n",
        "                                                        passes=10,\n",
        "                                                        random_state=1234,\n",
        "                                                        per_word_topics=True,\n",
        "                                                        workers=10)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqqcHIYB0FTK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df['lemmas'], start=2, limit=5, step=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy5QcDG90FTL"
      },
      "outputs": [],
      "source": [
        "start=2; limit=5;  step=1;\n",
        "x = range(start, limit, step)\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.grid()\n",
        "plt.title(\"Coherence Score vs. Number of Topics\")\n",
        "plt.xticks(x)\n",
        "plt.plot(x, coherence_values, \"-o\")\n",
        "\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcQ8QSlB0FTL"
      },
      "source": [
        "### Index for Model \n",
        "\n",
        "Due to the probabilistic nature of this model, the modeling results can and usually do vary. Despite this, we will select 8 as the number of topics even if this particular model run doesn't show 8 as having the highest coherence score. Also, even if it doesn't, we  need to ask ourselves how many topics we actually want for our corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNBBNNLk0FTL"
      },
      "outputs": [],
      "source": [
        "lda_trained_model = model_list[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBoX-rZ_0FTL"
      },
      "outputs": [],
      "source": [
        "lda_trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVmLb-9x0FTM"
      },
      "outputs": [],
      "source": [
        "# visualize the 3 topics \n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_trained_model, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku9RiruD0FTM"
      },
      "source": [
        "## Create a Topic id/name dictionary \n",
        "\n",
        "When populating your topic id/name dictionary, use the index ordering as shown in the viz tool. \n",
        "\n",
        "We'll use a function to map the the viz tool index ordering with the train LDA model ordering. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f578664ea9e81e93977a50dee417f2df",
          "grade": false,
          "grade_id": "cell-777be73d0d1455f0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "hp1JbnaZ0FTM"
      },
      "outputs": [],
      "source": [
        "# keys - use topic ids from pyLDAvis visualization \n",
        "# values - topic names that you create \n",
        "# save dictionary to `vis_topic_name_dict`\n",
        "# YOUR CODE HERE\n",
        "# raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plsG4Xfa0FTM"
      },
      "outputs": [],
      "source": [
        "def get_topic_id_lookup_dict(vis, vis_topic_name_dict):\n",
        "    \"\"\"\n",
        "    Both the starting index and the ordering of topic ids bewteen the trained LDA model \n",
        "    and the viz tool are different. So we need to create a look up dictionary that maps \n",
        "    the correct association between topic ids from both sources. \n",
        "    \"\"\"\n",
        "    # value is order of topic ids accoridng to pyLDAvis tool \n",
        "    # key is order of topic ids according to lda model\n",
        "    model_vis_tool_topic_id_lookup = vis.topic_coordinates.topics.to_dict()\n",
        "\n",
        "    # invert dictionary so that \n",
        "    # key is order of topic ids accoridng to pyLDAvis tool \n",
        "    # value is order of topic ids according to lda model\n",
        "    topic_id_lookup =  {v:k for k, v in model_vis_tool_topic_id_lookup.items()}\n",
        "    \n",
        "    # iterate through topic_id_lookup and index vis_topic_name_dict using the keys \n",
        "    # in order to swap the viz topic ids in vis_topic_name_dict for the lda model topic ids \n",
        "    return {v:vis_topic_name_dict[k]  for k, v in topic_id_lookup.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "92d9c30261d1cdb278e73f457c166609",
          "grade": false,
          "grade_id": "cell-0718245fd125b36e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "9R5pR6VB0FTN"
      },
      "outputs": [],
      "source": [
        "topic_name_dict = {0: \"Guns\", 1: \"Armenian_Turkish_Conflict\", 2: \"Space\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDa2vBAq0FTN"
      },
      "outputs": [],
      "source": [
        "topic_name_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju9_YBHX0FTN"
      },
      "source": [
        "## Assign Each Document a Topic Name\n",
        "\n",
        "Now that we have a topic id/name look up dict that is aligned with the index ordering of the trained LDA model, we can move forward to giving each topic a topic name. \n",
        "\n",
        "The function below has been given to you. However, you highly encouraged to read through it and make sure that you understand what it is doing each step of the way. In fact, a good way to do this is to copy and paste the code inside of the function into a new cell, comment out all the lines of code and line by line, uncomment the code and see the output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW6ygyua0FTN"
      },
      "outputs": [],
      "source": [
        "def get_topic_ids_for_docs(lda_model, corpus):\n",
        "    \n",
        "    \"\"\"\n",
        "    Passes a Bag-of-Words vector into a trained LDA model in order to get the topic id of that document. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    lda_model: Gensim object\n",
        "        Must be a trained model \n",
        "        \n",
        "    corpus: nested lists of tuples, \n",
        "        i.e. [[(),(), ..., ()], [(),(), ..., ()], ..., [(),(), ..., ()]]\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    topic_id_list: list\n",
        "        Contains topic ids for all document vectors in corpus \n",
        "    \"\"\"\n",
        "    \n",
        "    # store topic ids for each document\n",
        "    doc_topic_ids = []\n",
        "\n",
        "    # iterature through the bow vectors for each doc\n",
        "    for doc_bow in corpus:\n",
        "        \n",
        "        # store the topic ids for the doc\n",
        "        topic_ids = []\n",
        "        # store the topic probabilities for the doc\n",
        "        topic_probs = []\n",
        "\n",
        "        # list of tuples\n",
        "        # each tuple has a topic id and the prob that the doc belongs to that topic \n",
        "        topic_id_prob_tuples = lda_trained_model.get_document_topics(doc_bow)\n",
        "        \n",
        "        # iterate through the topic id/prob pairs \n",
        "        for topic_id_prob in topic_id_prob_tuples:\n",
        "            \n",
        "            # index for topic id\n",
        "            topic_id = topic_id_prob[0]\n",
        "            # index for prob that doc belongs that the corresponding topic\n",
        "            topic_prob = topic_id_prob[1]\n",
        "\n",
        "            # store all topic ids for doc\n",
        "            topic_ids.append(topic_id)\n",
        "            # store all topic probs for doc\n",
        "            topic_probs.append(topic_prob)\n",
        "\n",
        "        # get index for largest prob score\n",
        "        max_topic_prob_ind = np.argmax(topic_probs)\n",
        "        # get corresponding topic id\n",
        "        max_prob_topic_id = topic_ids[max_topic_prob_ind]\n",
        "        # store topic id that had the highest prob for doc being a memebr of that topic\n",
        "        doc_topic_ids.append(max_prob_topic_id)\n",
        "        \n",
        "    return doc_topic_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2d8uvnm0FTO"
      },
      "outputs": [],
      "source": [
        "# get the topic id for each doc in the corpus \n",
        "topic_id_list = get_topic_ids_for_docs(lda_trained_model, corpus)\n",
        "\n",
        "# creat a feature for document's topic id\n",
        "df[\"topic_id\"] = topic_id_list\n",
        "\n",
        "# iterate through the topic id and use the lookup table to assign each document with a topic name\n",
        "df[\"new_topic_name\"] = df[\"topic_id\"].apply(lambda topic_id: topic_name_dict[topic_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um5mt46R0FTO"
      },
      "outputs": [],
      "source": [
        "# cool! so now all of our documents have topic ids and names \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKDPqCgR0FTO"
      },
      "outputs": [],
      "source": [
        "# you can mask for all Space articles \n",
        "space_mask = df.topic_id == 2\n",
        "df[space_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV0CrCpq0FTO"
      },
      "source": [
        "-----\n",
        "\n",
        "## Where do we go from here?\n",
        "\n",
        "What exactly did we just accomplish?\n",
        "\n",
        "Outside of this guided project (i.e. in your job) you may or may not have access to existing article topic names like we did with this data set. Meaning that we won't always have a point of reference to \"check our answers\". So let's explore 2 possible situations in which you might find yourself using this Unsupervised Learning Topic Model. \n",
        "\n",
        "### 1. You have access to existing document topic labels\n",
        "\n",
        "In this case, why would we bother with Topic Modeling? It could be the case that the current topic labels are actually not helpful for whatever task you're working on. For instance, our email dataset here has topic names however those topic labels are hierarchical, which doesn't suit your needs for some reason. So one option is generate new labels that do suit your needs (like we did here). \n",
        "\n",
        "### 2. Your corpus doesn't have any document topic labels\n",
        "\n",
        "In this case, you don't have any pre-existing topic labels. Maybe you work at Indeed or LinkedIn or Google and your job is to bring some structure to a huge collection of emails and messages that aren't labeled in any meaningful way and so it's difficult to just sort through these documents. This is a perfect use case of Topic Modeling. After you apply topic modeling, you'll then have organized your emails into broad categories and you can start structuring and then analyze your corpus, and maybe even build a supervised learning model to predict the topic of the document!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DS33_414_Topic_Modeling_Lecture.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}