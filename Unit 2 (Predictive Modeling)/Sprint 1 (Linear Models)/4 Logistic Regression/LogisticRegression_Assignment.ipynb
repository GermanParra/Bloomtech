{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of LS_DS_214_solution.ipynb","provenance":[{"file_id":"https://github.com/LambdaSchool/DS-Unit-2-Linear-Models/blob/master/module4-logistic-regression/LS_DS_214_assignment.ipynb","timestamp":1635391235332}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RTGHPuw56c4b"},"source":["Lambda School Data Science\n","\n","*Unit 2, Sprint 1, Module 4*\n","\n","---"]},{"cell_type":"code","metadata":{"id":"mUSTagKP6c4e"},"source":["%%capture\n","import sys\n","\n","# If you're on Colab:\n","if 'google.colab' in sys.modules:\n","    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n","    !pip install category_encoders==2.*\n","\n","# If you're working locally:\n","else:\n","    DATA_PATH = '../data/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jl8FkGyk6c4g"},"source":["# Module Project: Logistic Regression\n","\n","Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n","\n","The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n","\n","## Directions\n","\n","The tasks for this project are the following:\n","\n","- **Task 1:** Import `csv` file using `wrangle` function.\n","- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n","- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n","- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n","- **Task 5:** Establish the baseline accuracy score for your dataset.\n","- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n","- **Task 7:** Calculate the training and test accuracy score for your model.\n","- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n","- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n","\n","**Note** \n","\n","You should limit yourself to the following libraries:\n","\n","- `category_encoders`\n","- `matplotlib`\n","- `pandas`\n","- `sklearn`"]},{"cell_type":"code","metadata":{"id":"diuXknb7KB_e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642861067262,"user_tz":300,"elapsed":1380,"user":{"displayName":"german Parra Tovar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ging63pR7hoN9eEKOI8icWt19Mg31JZ1KXpnimcfw=s64","userId":"11017221582952699346"}},"outputId":"90c6bfec-053a-4304-9480-7bd7064c2afc"},"source":["from category_encoders import OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"]}]},{"cell_type":"markdown","metadata":{"id":"d1uiIVon6c4h"},"source":["# I. Wrangle Data"]},{"cell_type":"code","metadata":{"id":"T-Csg-iM6c4h"},"source":["def wrangle(filepath):\n","    # Import w/ DateTimeIndex\n","    df = pd.read_csv(filepath, parse_dates=['Date'],\n","                     index_col='Date')\n","    \n","    # Drop unrated burritos\n","    df.dropna(subset=['overall'], inplace=True)\n","    \n","    # Derive binary classification target:\n","    # We define a 'Great' burrito as having an\n","    # overall rating of 4 or higher, on a 5 point scale\n","    df['Great'] = (df['overall'] >= 4).astype('object')\n","    \n","    # Drop high cardinality categoricals\n","    df = df.drop(columns=['Notes', 'Location', 'Address', 'URL', 'Neighborhood','Reviewer'])\n","    \n","    # Drop columns to prevent \"leakage\"\n","    df = df.drop(columns=['Rec', 'overall'])\n","\n","    # Drop Empty column\n","    df = df.drop(columns=['Queso'])\n","\n","    # Fixing Encoded Columns (1,0)\n","    for col in df: \n","      if df[col].nunique() <= 4:\n","       df[col].replace({\"X\":\"1\",\"x\":\"1\",\"Yes\":\"1\",\"No\":\"0\"}, inplace=True)\n","       df[col] = df[col].fillna('0')\n","       df[col] = df[col].astype(int)\n","\n","    # # # BURRITO COLUMN\n","    # # Fixing Strings\n","    # df['Burrito'] = df['Burrito'].str.lower().str.replace(\"\\\\s\",\"\")\n","    # # Encoding\n","    # one_hot = pd.get_dummies(df['Burrito'])\n","    # # Drop column Burrito as it is now encoded\n","    # df = df.drop('Burrito',axis = 1)\n","    # # Join the encoded df\n","    # df = df.join(one_hot)\n","\n","    Search = ['California', 'Asada', 'Surf', 'Carnitas']\n","    df[Search] = 0\n","    df.loc[df['Burrito'].str.contains('California'), 'California'] = 1\n","    df.loc[df['Burrito'].str.contains('asada'), 'Asada'] = 1\n","    df.loc[df['Burrito'].str.contains('surf'), 'Surf'] = 1\n","    df.loc[df['Burrito'].str.contains('Carnitas'), 'Carnitas'] = 1\n","    df.drop(columns='Burrito', inplace=True)\n","\n","    return df\n","\n","filepath = DATA_PATH + 'burritos/burritos.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57N1o3ST6c4i"},"source":["**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."]},{"cell_type":"code","metadata":{"id":"5Cip3cj_6c4j"},"source":["filepath = DATA_PATH + 'burritos/burritos.csv'\n","df = wrangle(filepath)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hp5qTNyM6c4k"},"source":["During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n","\n","```\n","0      x\n","1      x\n","2    NaN\n","3      x\n","4      x\n","Name: Beef, dtype: object\n","```\n","\n","**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."]},{"cell_type":"code","metadata":{"id":"e1U4KqJC6c4k","colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"status":"ok","timestamp":1642861105232,"user_tz":300,"elapsed":161,"user":{"displayName":"german Parra Tovar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ging63pR7hoN9eEKOI8icWt19Mg31JZ1KXpnimcfw=s64","userId":"11017221582952699346"}},"outputId":"eccfc12c-d02a-4542-ee0d-cb9412d30aa8"},"source":["# Conduct your exploratory data analysis here\n","# And modify the `wrangle` function above.\n","\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-300c821e-3fa6-4797-9ed1-b347a200a842\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Yelp</th>\n","      <th>Google</th>\n","      <th>Chips</th>\n","      <th>Cost</th>\n","      <th>Hunger</th>\n","      <th>Mass (g)</th>\n","      <th>Density (g/mL)</th>\n","      <th>Length</th>\n","      <th>Circum</th>\n","      <th>Volume</th>\n","      <th>Tortilla</th>\n","      <th>Temp</th>\n","      <th>Meat</th>\n","      <th>Fillings</th>\n","      <th>Meat:filling</th>\n","      <th>Uniformity</th>\n","      <th>Salsa</th>\n","      <th>Synergy</th>\n","      <th>Wrap</th>\n","      <th>Unreliable</th>\n","      <th>NonSD</th>\n","      <th>Beef</th>\n","      <th>Pico</th>\n","      <th>Guac</th>\n","      <th>Cheese</th>\n","      <th>Fries</th>\n","      <th>Sour cream</th>\n","      <th>Pork</th>\n","      <th>Chicken</th>\n","      <th>Shrimp</th>\n","      <th>Fish</th>\n","      <th>Rice</th>\n","      <th>Beans</th>\n","      <th>Lettuce</th>\n","      <th>Tomato</th>\n","      <th>Bell peper</th>\n","      <th>Carrots</th>\n","      <th>Cabbage</th>\n","      <th>Sauce</th>\n","      <th>Salsa.1</th>\n","      <th>Cilantro</th>\n","      <th>Onion</th>\n","      <th>Taquito</th>\n","      <th>Pineapple</th>\n","      <th>Ham</th>\n","      <th>Chile relleno</th>\n","      <th>Nopales</th>\n","      <th>Lobster</th>\n","      <th>Egg</th>\n","      <th>Mushroom</th>\n","      <th>Bacon</th>\n","      <th>Sushi</th>\n","      <th>Avocado</th>\n","      <th>Corn</th>\n","      <th>Zucchini</th>\n","      <th>Great</th>\n","      <th>California</th>\n","      <th>Asada</th>\n","      <th>Surf</th>\n","      <th>Carnitas</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2016-01-18</th>\n","      <td>3.5</td>\n","      <td>4.2</td>\n","      <td>0</td>\n","      <td>6.49</td>\n","      <td>3.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>3.5</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-24</th>\n","      <td>3.5</td>\n","      <td>3.3</td>\n","      <td>0</td>\n","      <td>5.45</td>\n","      <td>3.5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>2.0</td>\n","      <td>3.5</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>3.5</td>\n","      <td>2.5</td>\n","      <td>5.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-24</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>4.85</td>\n","      <td>1.5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.5</td>\n","      <td>3.0</td>\n","      <td>4.5</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-24</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>5.25</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-27</th>\n","      <td>4.0</td>\n","      <td>3.8</td>\n","      <td>1</td>\n","      <td>6.59</td>\n","      <td>4.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>3.5</td>\n","      <td>4.5</td>\n","      <td>5.0</td>\n","      <td>2.5</td>\n","      <td>4.5</td>\n","      <td>4.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-300c821e-3fa6-4797-9ed1-b347a200a842')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-300c821e-3fa6-4797-9ed1-b347a200a842 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-300c821e-3fa6-4797-9ed1-b347a200a842');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["            Yelp  Google  Chips  Cost  ...  California  Asada  Surf  Carnitas\n","Date                                   ...                                   \n","2016-01-18   3.5     4.2      0  6.49  ...           1      0     0         0\n","2016-01-24   3.5     3.3      0  5.45  ...           1      0     0         0\n","2016-01-24   NaN     NaN      0  4.85  ...           0      0     0         1\n","2016-01-24   NaN     NaN      0  5.25  ...           0      1     0         0\n","2016-01-27   4.0     3.8      1  6.59  ...           1      0     0         0\n","\n","[5 rows x 60 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"Twa-bCpH6c4l"},"source":["If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n","\n","**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n","\n","| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n","| :---------- | :------------: | :-------: | :------: | :----------: |\n","| California  |       1        |     0     |    0     |      0       |\n","| California  |       1        |     0     |    0     |      0       |\n","|  Carnitas   |       0        |     0     |    0     |      1       |\n","| Carne asada |       0        |     1     |    0     |      0       |\n","| California  |       1        |     0     |    0     |      0       |\n","\n","**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."]},{"cell_type":"code","metadata":{"id":"2VpgYxgf6c4m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642861250573,"user_tz":300,"elapsed":134,"user":{"displayName":"german Parra Tovar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ging63pR7hoN9eEKOI8icWt19Mg31JZ1KXpnimcfw=s64","userId":"11017221582952699346"}},"outputId":"a2113fc0-2276-43f6-b6af-e4b7be8c347d"},"source":["# Conduct your exploratory data analysis here\n","# And modify the `wrangle` function above.\n","df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","DatetimeIndex: 421 entries, 2016-01-18 to 2019-08-27\n","Data columns (total 60 columns):\n"," #   Column          Non-Null Count  Dtype  \n","---  ------          --------------  -----  \n"," 0   Yelp            87 non-null     float64\n"," 1   Google          87 non-null     float64\n"," 2   Chips           421 non-null    int64  \n"," 3   Cost            414 non-null    float64\n"," 4   Hunger          418 non-null    float64\n"," 5   Mass (g)        22 non-null     float64\n"," 6   Density (g/mL)  22 non-null     float64\n"," 7   Length          283 non-null    float64\n"," 8   Circum          281 non-null    float64\n"," 9   Volume          281 non-null    float64\n"," 10  Tortilla        421 non-null    float64\n"," 11  Temp            401 non-null    float64\n"," 12  Meat            407 non-null    float64\n"," 13  Fillings        418 non-null    float64\n"," 14  Meat:filling    412 non-null    float64\n"," 15  Uniformity      419 non-null    float64\n"," 16  Salsa           396 non-null    float64\n"," 17  Synergy         419 non-null    float64\n"," 18  Wrap            418 non-null    float64\n"," 19  Unreliable      421 non-null    int64  \n"," 20  NonSD           421 non-null    int64  \n"," 21  Beef            421 non-null    int64  \n"," 22  Pico            421 non-null    int64  \n"," 23  Guac            421 non-null    int64  \n"," 24  Cheese          421 non-null    int64  \n"," 25  Fries           421 non-null    int64  \n"," 26  Sour cream      421 non-null    int64  \n"," 27  Pork            421 non-null    int64  \n"," 28  Chicken         421 non-null    int64  \n"," 29  Shrimp          421 non-null    int64  \n"," 30  Fish            421 non-null    int64  \n"," 31  Rice            421 non-null    int64  \n"," 32  Beans           421 non-null    int64  \n"," 33  Lettuce         421 non-null    int64  \n"," 34  Tomato          421 non-null    int64  \n"," 35  Bell peper      421 non-null    int64  \n"," 36  Carrots         421 non-null    int64  \n"," 37  Cabbage         421 non-null    int64  \n"," 38  Sauce           421 non-null    int64  \n"," 39  Salsa.1         421 non-null    int64  \n"," 40  Cilantro        421 non-null    int64  \n"," 41  Onion           421 non-null    int64  \n"," 42  Taquito         421 non-null    int64  \n"," 43  Pineapple       421 non-null    int64  \n"," 44  Ham             421 non-null    int64  \n"," 45  Chile relleno   421 non-null    int64  \n"," 46  Nopales         421 non-null    int64  \n"," 47  Lobster         421 non-null    int64  \n"," 48  Egg             421 non-null    int64  \n"," 49  Mushroom        421 non-null    int64  \n"," 50  Bacon           421 non-null    int64  \n"," 51  Sushi           421 non-null    int64  \n"," 52  Avocado         421 non-null    int64  \n"," 53  Corn            421 non-null    int64  \n"," 54  Zucchini        421 non-null    int64  \n"," 55  Great           421 non-null    int64  \n"," 56  California      421 non-null    int64  \n"," 57  Asada           421 non-null    int64  \n"," 58  Surf            421 non-null    int64  \n"," 59  Carnitas        421 non-null    int64  \n","dtypes: float64(18), int64(42)\n","memory usage: 200.6 KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"f8yzJd016c4n"},"source":["# II. Split Data\n","\n","**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."]},{"cell_type":"code","metadata":{"id":"7xozNzFt6c4n"},"source":["X = df.drop(columns='Great')\n","y = df['Great']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"km0tdruL6c4o"},"source":["**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n","\n","- Your training set should include data from 2016 through 2017. \n","- Your test set should include data from 2018 and later."]},{"cell_type":"code","metadata":{"id":"CXfTIKAN6c4o"},"source":["mask = (df.index >= \"2016-01-01\") & (df.index < \"2018-01-01\")\n","\n","X_train, y_train = X.loc[mask], y.loc[mask]\n","X_test, y_test = X.loc[~mask], y.loc[~mask]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mQFZoZLI6c4o"},"source":["# III. Establish Baseline\n","\n","**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "]},{"cell_type":"code","metadata":{"id":"3D_kmO2w6c4p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642861297845,"user_tz":300,"elapsed":127,"user":{"displayName":"german Parra Tovar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ging63pR7hoN9eEKOI8icWt19Mg31JZ1KXpnimcfw=s64","userId":"11017221582952699346"}},"outputId":"4af6d410-e6f8-4d85-cd22-6bf9c84cb3f5"},"source":["baseline_acc = y_train.value_counts(normalize=True).max()\n","print('Baseline Accuracy Score:', round(baseline_acc*100, 2),'%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline Accuracy Score: 58.27 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"9k4u-0dg6c4p"},"source":["# IV. Build Model\n","\n","**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n","\n","- a `OneHotEncoder` transformer for categorical features, \n","- a `SimpleImputer` transformer to deal with missing values, \n","- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n","- a `LogisticRegression` predictor."]},{"cell_type":"code","metadata":{"id":"KU--XSV96c4p"},"source":["model_logr = make_pipeline(\n","    #OneHotEncoder(use_cat_names=True),\n","    SimpleImputer(strategy='mean'),\n","    StandardScaler(),\n","    LogisticRegression()\n",")\n","\n","model_logr.fit(X_train,y_train);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdu47HIt6c4p"},"source":["# IV. Check Metrics\n","\n","**Task 7:** Calculate the training and test accuracy score for `model_lr`."]},{"cell_type":"code","metadata":{"id":"xcxbjZVc6c4q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642861803471,"user_tz":300,"elapsed":14,"user":{"displayName":"german Parra Tovar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ging63pR7hoN9eEKOI8icWt19Mg31JZ1KXpnimcfw=s64","userId":"11017221582952699346"}},"outputId":"0eb5d8f4-d402-4554-f4b5-34e6a9a6c2dc"},"source":["training_acc = model_logr.score(X_train, y_train)\n","test_acc = model_logr.score(X_test,y_test)\n","\n","print('Training Accuracy:', round(training_acc*100, 2),'%')\n","print('Test Accuracy:', round(test_acc*100, 2),'%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training MAE: 91.34 %\n","Test MAE: 75.0 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"I11pxqc86c4q"},"source":["# V. Communicate Results\n","\n","**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n","\n","**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."]},{"cell_type":"code","metadata":{"id":"SIFQ5jQs6c4q","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1642861906768,"user_tz":300,"elapsed":251,"user":{"displayName":"german Parra Tovar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ging63pR7hoN9eEKOI8icWt19Mg31JZ1KXpnimcfw=s64","userId":"11017221582952699346"}},"outputId":"925b52cb-78cd-4cb3-ce6c-eec3cceb07e1"},"source":["# Create your horizontal barchart here.\n","\n","Coefficients = model_logr.named_steps.logisticregression.coef_.flatten()\n","Features = X_train.columns #model_logr.named_steps.onehotencoder.get_feature_names()\n","\n","Best_features = pd.Series(data=Coefficients, index=Features).sort_values(key=abs)\n","Best_features.tail(10).plot(kind='barh', color='green')\n","plt.title('10 Most Important Coefficients');"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"TNWWHB1v6c4q"},"source":["There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n","\n","**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n","\n","- What data type do `predict` and `predict_proba` output?\n","- What are the shapes of their different output?\n","- What numerical values are in the output?\n","- What do those numerical values represent?"]},{"cell_type":"code","metadata":{"id":"UsVn3zJt6c4r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642861908217,"user_tz":300,"elapsed":109,"user":{"displayName":"german Parra Tovar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ging63pR7hoN9eEKOI8icWt19Mg31JZ1KXpnimcfw=s64","userId":"11017221582952699346"}},"outputId":"038f41b2-73e4-445d-c7aa-5050bca6a72b"},"source":["# Write code here to explore the differences between `predict` and `predict_proba`.\n","\n","model_logr.predict(X_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n","       0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDVpvgxoTbg1","executionInfo":{"status":"ok","timestamp":1642862022275,"user_tz":300,"elapsed":113,"user":{"displayName":"german Parra Tovar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ging63pR7hoN9eEKOI8icWt19Mg31JZ1KXpnimcfw=s64","userId":"11017221582952699346"}},"outputId":"b2dd84b3-d414-4514-ad1f-3d034c68b880"},"source":["model_logr.predict_proba(X_test)[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[4.63785865e-05, 9.99953621e-01],\n","       [9.99968710e-01, 3.12899815e-05],\n","       [3.74843758e-05, 9.99962516e-01],\n","       [2.47218351e-03, 9.97527816e-01],\n","       [5.35521960e-01, 4.64478040e-01],\n","       [1.57196635e-05, 9.99984280e-01],\n","       [9.98545632e-01, 1.45436762e-03],\n","       [8.60810365e-01, 1.39189635e-01],\n","       [3.64548384e-02, 9.63545162e-01],\n","       [4.10559229e-04, 9.99589441e-01]])"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"L8znJ5YZ6c4r"},"source":["**Give your written answer here:**\n","\n","\n","* What data type do `predict` and `predict_proba` output?\n","* What numerical values are in the output?\n","* What do those numerical values represent?\n","\n","**predict: A binary list of prediction\\\n","predict_proba: 2 column array with the probabilities for each category (1 or 0)**\n","\n","* What are the shapes of their different output?\\\n","\n","predict: 1D array\\\n","  predict_proba: 2 column array (2D)\n","\n","\n","\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n","X = imp_mean.fit_transform(X)\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# build and fit model\n","early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n","    monitor='loss', min_delta=0.0002, patience=1, verbose=0,\n","    mode='auto', baseline=None, restore_best_weights=True\n",")\n","# YOUR CODE HERE\n","model2 = Sequential()\n","\n","model2.add(Dense(32, 'relu'))\n","model2.add(Dense(20, 'relu'))\n","model2.add(Dense(1, 'sigmoid'))\n","\n","model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model2.fit(X, y, epochs=100, callbacks=[early_stopping_callback])\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8o6R2kRrPMRE","executionInfo":{"status":"ok","timestamp":1642864298936,"user_tz":300,"elapsed":3246,"user":{"displayName":"german Parra Tovar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ging63pR7hoN9eEKOI8icWt19Mg31JZ1KXpnimcfw=s64","userId":"11017221582952699346"}},"outputId":"c90791e4-83ba-4c84-bbb5-a4f8e78b0d7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","14/14 [==============================] - 1s 2ms/step - loss: 0.6795 - accuracy: 0.6033\n","Epoch 2/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.6983\n","Epoch 3/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7553\n","Epoch 4/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.7910\n","Epoch 5/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.8195\n","Epoch 6/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8480\n","Epoch 7/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8599\n","Epoch 8/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8812\n","Epoch 9/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.3130 - accuracy: 0.8884\n","Epoch 10/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8979\n","Epoch 11/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.2731 - accuracy: 0.8955\n","Epoch 12/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.2558 - accuracy: 0.9002\n","Epoch 13/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.2417 - accuracy: 0.9097\n","Epoch 14/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.9145\n","Epoch 15/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.9169\n","Epoch 16/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.2068 - accuracy: 0.9216\n","Epoch 17/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1989 - accuracy: 0.9240\n","Epoch 18/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1907 - accuracy: 0.9359\n","Epoch 19/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1830 - accuracy: 0.9359\n","Epoch 20/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1762 - accuracy: 0.9406\n","Epoch 21/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1685 - accuracy: 0.9406\n","Epoch 22/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.9406\n","Epoch 23/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1566 - accuracy: 0.9406\n","Epoch 24/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.1522 - accuracy: 0.9430\n","Epoch 25/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1458 - accuracy: 0.9430\n","Epoch 26/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9525\n","Epoch 27/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1369 - accuracy: 0.9501\n","Epoch 28/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.1318 - accuracy: 0.9525\n","Epoch 29/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9501\n","Epoch 30/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.1239 - accuracy: 0.9549\n","Epoch 31/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.9572\n","Epoch 32/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9596\n","Epoch 33/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1135 - accuracy: 0.9596\n","Epoch 34/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.1098 - accuracy: 0.9620\n","Epoch 35/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9691\n","Epoch 36/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9715\n","Epoch 37/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9667\n","Epoch 38/100\n","14/14 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9691\n","Epoch 39/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.0934 - accuracy: 0.9715\n","Epoch 40/100\n","14/14 [==============================] - 0s 2ms/step - loss: 0.0936 - accuracy: 0.9691\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fbc0e3cc2d0>"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":[""],"metadata":{"id":"p_Dy8_ZuTHU8"},"execution_count":null,"outputs":[]}]}